# TP2 : Ansible

Le but de ce TP est d'approfondir l'utilisation d'Ansible :

- construction de playbooks
- organisation de dÃ©pÃ´t
- workflow de travail

> Il est strictement nÃ©cessaire d'avoir terminÃ© les [TP0]() et [TP1]().

# Sommaire

- [TP2 : Ansible](#tp2--ansible)
- [Sommaire](#sommaire)
- [0. Setup](#0-setup)
- [I. Init repo](#i-init-repo)
- II. Un dÃ©pÃ´t Ansible rangÃ©
  - [1. Structure du dÃ©pÃ´t : inventaires](#1-structure-du-dÃ©pÃ´t--inventaires)
  - [2. Structure du dÃ©pÃ´t : rÃ´les](#2-structure-du-dÃ©pÃ´t--rÃ´les)
  - [3. Structure du dÃ©pÃ´t : variables d'inventaire](#3-structure-du-dÃ©pÃ´t--variables-dinventaire)
  - [4. Structure du dÃ©pÃ´t : rÃ´le avancÃ©](#4-structure-du-dÃ©pÃ´t--rÃ´le-avancÃ©)
  - [5. GÃ©rer la suppression](#5-gÃ©rer-la-suppression)
- III. Repeat
  - [1. NGINX](#1-nginx)
  - [2. Common](#2-common)
  - [3. Dynamic Loadbalancer](#3-dynamic-loadbalancer)
- IV. Aller plus loin
  - [1. Vault Ansible](#1-vault-ansible)
  - [2. Support de plusieurs OS](#2-support-de-plusieurs-os)

# 0. Setup

> Je vous laisse le choix des OS GNU/Linux.

Pour rÃ©aliser le TP vous allez avoir besoin de :

- 1 poste avec Ansible : 

  le *control node*

  - votre PC sous Linux, ou une VM sous Linux

  - le fichier 

    ```
    hosts
    ```

     de cette machine doit Ãªtre rempli pour pouvoir joindre les deux 

    managed nodes

     avec des noms plutÃ´t qu'une IP :

    - `node1.tp2.cloud`
    - `node2.tp2.cloud`

- 2 machines Linux : 

  les *managed nodes*

  - dÃ©ployÃ©es avec Vagrant

    - dÃ©finissez leur une IP statique

  - prÃ©configurÃ©es avec 

    ```
    cloud-init
    ```

    - un utilisateur crÃ©Ã©
    - cet utilisateur a accÃ¨s aux droits de `root` *via* la commande `sudo`
    - dÃ©posez une clÃ© SSH sur un utilisateur que vous avez crÃ©Ã©

- un dÃ©pÃ´t git dans lequel on stockera notre code Ansible

  - pour le moment, on va faire simple, et vous pouvez utilisez un fournisseur public comme Gitlab ou Github

Une fois les machines en place, assurez-vous que vous avez avoir une connexion SSH sans mot **de passe depuis le \*control node\* vers les \*managed nodes\***.

------

ðŸŒž **Je veux un seul truc dans le rendu Markdown : le lien vers votre dÃ©pÃ´t git qui contient tout le code Ansible**



# I. Init repo

âžœ **CrÃ©ez un rÃ©pertoire de travail**

- sur le *control node*
- dans le *home directory* de l'utilisateur que vous utilisez
- ce rÃ©pertoire doit Ãªtre un dÃ©pÃ´t git que vous avez crÃ©Ã© sur une plateforme publique comme Gitlab, Github, etc.

> **Tous les fichiers Ansible devront Ãªtre crÃ©Ã©s dans de dossier.**

âžœ **CrÃ©ez le fichier d'inventaire Ansible**

- rÃ©fÃ©rez-vous au [TP0]()

- crÃ©ez un fichier 

  inventory

  ```
  hosts.ini
  ```

  - nommez le groupe d'hÃ´tes `ynov`
  - les instructions du TP utiliseront `ynov` comme nom de groupe

- utilisez le module `ping` de Ansible pour tester qu'Ansible peut joindre les machines

```shell
$ ansible ynov -i hosts.ini -m ping 
```



âžœ **CrÃ©ez un playbook de test**

- dans le rÃ©pertoire de travail Ansible, crÃ©ez un sous-rÃ©pertoire `playbooks/`

- crÃ©ez un fichier `playbooks/test.yml`

- Ã©crire le nÃ©cessaire dans le fichier pour installer 

  ```
  vim
  ```

   sur les 

  managed nodes

  - rÃ©fÃ©rez-vous au [fichier `nginx.yml` du TP0]()

# II. Un dÃ©pÃ´t Ansible rangÃ©

## 1. Structure du dÃ©pÃ´t : inventaires

âžœ **Dans votre rÃ©pertoire de travail Ansible...**

- crÃ©ez un rÃ©pertoire `inventories/`
- crÃ©ez un rÃ©pertoire `inventories/vagrant_lab/`
- dÃ©placez le fichier `hosts.ini` dans `inventories/vagrant_lab/hosts.ini`
- assurez vous que pouvez toujours dÃ©ployer correctement avec une commande `ansible-playbook`

```bash
âžœ  tp2-ansible git:(master) âœ— ansible-playbook -i Ansible/inventories/vagrant_lab/hosts.ini Ansible/test.yml

PLAY [Install vim] *********************************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************************
The authenticity of host 'node2.tp2.cloud (192.168.56.5)' can't be established.
ECDSA key fingerprint is SHA256:NS8ww4SMoWGIODdUNPWy2Vwt9bcakE9fL7zk2JGlzPbw.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
The authenticity of host 'node1.tp2.cloud (192.168.56.4)' can't be established.
ECDSA key fingerprint is SHA256:FP/tYR0hUuiE85XpUHkiNv0HXC2XHJwYu+9bcakcsYwM.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
ok: [node2.tp2.cloud]
ok: [node1.tp2.cloud]

TASK [apt-get update] ******************************************************************************************************************************************
ok: [node1.tp2.cloud]
ok: [node2.tp2.cloud]

TASK [install vim] *********************************************************************************************************************************************
ok: [node1.tp2.cloud]
ok: [node2.tp2.cloud]

PLAY RECAP *****************************************************************************************************************************************************
node1.tp2.cloud            : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2.tp2.cloud            : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

```

## 2. Structure du dÃ©pÃ´t : rÃ´les

**Les \*rÃ´les\* permettent de regrouper de faÃ§on logique les diffÃ©rentes configurations qu'un dÃ©pÃ´t Ansible contient.**

Un *rÃ´le* correspond Ã  une configuration spÃ©cifique, ou une application spÃ©ficique. Ainsi on peut trouver un rÃ´le `apache` qui installe le serveur web Apache, ou encore `mysql`, `nginx`, etc.

**Un rÃ´le doit Ãªtre gÃ©nÃ©rique.** Il ne doit pas Ãªtre spÃ©cifique Ã  telle ou telle machine.

Il existe des conventions et bonnes pratiques pour structurer les *rÃ´les* Ansible, que nous allons voir dans cette partie.

On crÃ©e souvent un *rÃ´le* `common` qui est appliquÃ© sur toutes les machines du parc, et qui pose la configuration Ã©lÃ©mentaire, commune Ã  toutes les machines.

âžœ **Ajout d'un fichier de config Ansible**

- dans le rÃ©pertoire de travail, crÃ©ez un fichier `ansible.cfg` :

```ini
[defaults]
roles_path = ./roles
```



âžœ **Dans votre rÃ©pertoire de travail Ansible...**

- crÃ©ez un rÃ©pertoire `roles/`
- crÃ©ez un rÃ©pertoire `roles/common/`
- crÃ©ez un rÃ©pertoire `roles/common/tasks/`
- crÃ©ez un fichier `roles/common/tasks/main.yml` avec le contenu suivant :

```yaml
- name: Install common packages
  import_tasks: packages.yml
```



- crÃ©ez un fichier 

  ```
  roles/common/tasks/packages.yml
  ```

   :

  - on va en profiter pour manipuler des variables Ansible

```yaml
- name: Install common packages
  ansible.builtin.package:
    name: "{{ item }}"
    state: present
  with_items: "{{ common_packages }}" # ceci permet de boucler sur la liste common_packages
```



- crÃ©ez un rÃ©pertoire `roles/common/defaults/`
- crÃ©ez un fichier `roles/common/defaults/main.yml` :

```yaml
common_packages:
  - vim
  - git
```



- crÃ©ez un fichier `playbooks/main.yml`

```yaml
- hosts: ynov
  roles:
    - common
```



âžœ **Testez d'appliquer ce playbook avec une commande `ansible-playbook`**

## 3. Structure du dÃ©pÃ´t : variables d'inventaire

Afin de garder la complexitÃ© d'un dÃ©pÃ´t Ansible sous contrÃ´le, il est rÃ©current d'user et abuser de l'utilisation des variables.

Il est possible dans un dÃ©pÃ´t Ansible de dÃ©clarer Ã  plusieurs endroits : on a dÃ©jÃ  vu le rÃ©pertoire `defaults/` Ã  l'intÃ©rieur d'un rÃ´le (comme notre `roles/common/defaults/`) que l'on a crÃ©Ã© juste avant. Ce rÃ©pertoire est utile pour dÃ©clarer des variables spÃ©cifiques au rÃ´le.

Qu'en est-il, dans notre cas prÃ©sent, si l'on souhaite installer un paquet sur une seule machine, mais qui est considÃ©rÃ© comme un paquet de "base" ? On aimerait l'ajouter dans la liste dans `roles/common/defaults/main.yml` mais ce serait moche d'avoir une condition sur le nom de la machine Ã  cet endroit (un rÃ´le doit Ãªtre gÃ©nÃ©rique).

**Pour cela on utilise les `host_vars`.**

âžœ **Dans votre rÃ©pertoire de travail Ansible...**

- crÃ©ez un rÃ©pertoire `inventories/vagrant_lab/host_vars/`
- crÃ©ez un fichier `inventories/vagrant_lab/host_vars/node1.tp2.cloud.yml` :

```yaml
common_packages:
  - vim
  - git
  - rsync
```



âžœ **Testez d'appliquer le playbook avec une commande `ansible-playbook`**

```bash
âžœ  Ansible git:(master) âœ— ansible-playbook -i inventories/vagrant_lab/hosts.ini playbooks/main.yml


PLAY [ynov] ****************************************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************************
The authenticity of host 'node2.tp2.cloud (192.168.56.5)' can't be established.
ECDSA key fingerprint is SHA256:NS8ww4SMoWGIODdUNPWy2Vwt9bcakE9fL7zk2JGlzPbw.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
The authenticity of host 'node1.tp2.cloud (192.168.56.4)' can't be established.
ECDSA key fingerprint is SHA256:FP/tYR0hUuiE85XpUHkiNv0HXC2XHJwYu+9bcakusYwM.
Are you sure you want to continue connecting (yes/no/[fingerprint])? ok: [node2.tp2.cloud]
yes
ok: [node1.tp2.cloud]

TASK [common : Install common packages] ************************************************************************************************************************
ok: [node2.tp2.cloud] => (item=vim)
ok: [node1.tp2.cloud] => (item=vim)
ok: [node2.tp2.cloud] => (item=git)
ok: [node1.tp2.cloud] => (item=git)
ok: [node1.tp2.cloud] => (item=rsync)

PLAY RECAP *****************************************************************************************************************************************************
node1.tp2.cloud            : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2.tp2.cloud            : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
```



------

Il est aussi possible d'attribuer des variables Ã  un groupe de machines dÃ©finies dans l'inventaire. **On utilise pour Ã§a les `group_vars`.**

âžœ **Dans votre rÃ©pertoire de travail Ansible...**

- crÃ©ez un rÃ©pertoire `inventories/vagrant_lab/group_vars/`
- crÃ©ez un fichier `inventories/vagrant_lab/group_vars/ynov.yml` :

```yaml
users:
  - le_nain
  - l_elfe
  - le_ranger
```



âžœ **Modifiez le fichier `roles/common/tasks/main.yml`** pour inclure un nouveau fichier  `roles/common/tasks/users.yml` :

- il doit utiliser cette variable `users` pour crÃ©er des utilisateurs
- rÃ©utilisez la syntaxe avec le `with_items`
- la variable `users` est accessible, du moment que vous dÃ©ployez sur les machines qui sont dans le groupe `ynov`

âžœ **VÃ©rifiez la bonne exÃ©cution du playbook**

```bash
âžœ  Ansible git:(master) âœ— ansible-playbook -i inventories/vagrant_lab/hosts.ini playbooks/main.yml


PLAY [ynov] ****************************************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************************
ok: [node1.tp2.cloud]
ok: [node2.tp2.cloud]

TASK [common : Install common packages] ************************************************************************************************************************
ok: [node1.tp2.cloud] => (item=vim)
ok: [node2.tp2.cloud] => (item=vim)
ok: [node1.tp2.cloud] => (item=git)
ok: [node2.tp2.cloud] => (item=git)
ok: [node1.tp2.cloud] => (item=rsync)

TASK [common : Create a users attached to ynov group] **********************************************************************************************************
changed: [node2.tp2.cloud] => (item=le_nain)
changed: [node1.tp2.cloud] => (item=le_nain)
changed: [node1.tp2.cloud] => (item=l_elfe)
changed: [node2.tp2.cloud] => (item=l_elfe)
changed: [node2.tp2.cloud] => (item=le_ranger)
[WARNING]: The input password appears not to have been hashed. The 'password' argument must be encrypted for this module to work properly.
changed: [node1.tp2.cloud] => (item=le_ranger)

PLAY RECAP *****************************************************************************************************************************************************
node1.tp2.cloud            : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2.tp2.cloud            : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
```

```bash
vagrant@node1:~$ cat /etc/passwd | grep 'le_nain\|l_elfe\|le_ranger'
le_nain:x:1004:1006::/home/le_nain:/bin/sh
l_elfe:x:1005:1007::/home/l_elfe:/bin/sh
le_ranger:x:1006:1008::/home/le_ranger:/bin/sh
```



## 4. Structure du dÃ©pÃ´t : rÃ´le avancÃ©

âžœ **CrÃ©ez un nouveau rÃ´le `nginx`**

- crÃ©ez le rÃ©pertoire du rÃ´le `roles/nginx/`
- crÃ©ez un sous-rÃ©pertoire `roles/nginx/tasks/` et un fichier `main.yml` Ã  l'intÃ©rieur :

```yaml
- name: Install NGINX
  import_tasks: install.yml

- name: Configure NGINX
  import_tasks: config.yml

- name: Deploy VirtualHosts
  import_tasks: vhosts.yml
```



âžœ **Remplissez le fichier `roles/nginx/tasks/install.yml`**

- il doit installer le paquet NGINX
  - je vous laisse gÃ©rer :)
  
  ```yaml
  - name: Install NGINX
    become: yes
    ansible.builtin.package:
      name: nginx
      state: present
  ```

âžœ **On va y ajouter quelques mÃ©caniques : fichiers et templates :**

- crÃ©ez un rÃ©pertoire `roles/nginx/files/`

- crÃ©ez un fichier 

  ```
  roles/nginx/files/nginx.conf
  ```

  - rÃ©cupÃ©rez un fichier `nginx.conf` par dÃ©faut (en faisant une install Ã  la main par exemple)
  - ajoutez une ligne `include conf.d/*.conf;`

- crÃ©ez un rÃ©pertoire `roles/nginx/templates/`

- crÃ©ez un fichier 

  ```
  roles/nginx/templates/vhost.conf.j2
  ```

   :

  - `.j2` c'pour Jinja2, c'est le nom du moteur de templating utilisÃ© par Ansible

```nginx
server {
        listen {{ nginx_port }} ;
        server_name {{ nginx_servername }};

        location / {
            root {{ nginx_webroot }};
            index index.html;
        }
}
```



âžœ **Remplissez le fichier `roles/nginx/tasks/config.yml`** :

```yaml
- name : Main NGINX config file
  copy:
    src: nginx.conf # pas besoin de prÃ©ciser de path, il sait qu'il doit chercher dans le dossier files/
    dest: /etc/nginx/nginx.conf
```



âžœ **Quelques variables `roles/nginx/defaults/main.yml`** :

```yaml
nginx_servername: test
nginx_port: 8080
nginx_webroot: /var/www/html/test
nginx_index_content: "<h1>teeeeeest</h1>"
```



âžœ **Remplissez le fichier `roles/nginx/tasks/vhosts.yml`** :

```yaml
- name: Create webroot
  file:
    path: "{{ nginx_webroot }}"
    state: directory

- name: Create index
  copy:
    dest: "{{ nginx_webroot }}/index.html"
    content: "{{ nginx_index_content }}"

- name: NGINX Virtual Host
  template:
    src: vhost.conf.j2
    dest: /etc/nginx/conf.d/{{ nginx_servername }}.conf
```



âžœ **Deploy !**

- ajoutez ce rÃ´le `nginx` au playbook
- et dÃ©ployez avec une commande `ansible-playbook`

```bash
âžœ  Ansible git:(master) âœ— ansible-playbook -i inventories/vagrant_lab/hosts.ini playbooks/main.yml 


PLAY [ynov] ****************************************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************************
ok: [node2.tp2.cloud]
ok: [node1.tp2.cloud]

TASK [common : Install common packages] ************************************************************************************************************************
ok: [node2.tp2.cloud] => (item=vim)
ok: [node1.tp2.cloud] => (item=vim)
ok: [node2.tp2.cloud] => (item=git)
ok: [node1.tp2.cloud] => (item=git)
ok: [node1.tp2.cloud] => (item=rsync)

TASK [common : Create a users attached to ynov group] **********************************************************************************************************
ok: [node2.tp2.cloud] => (item=le_nain)
ok: [node1.tp2.cloud] => (item=le_nain)
ok: [node2.tp2.cloud] => (item=l_elfe)
ok: [node1.tp2.cloud] => (item=l_elfe)
ok: [node2.tp2.cloud] => (item=le_ranger)
ok: [node1.tp2.cloud] => (item=le_ranger)

TASK [nginx : Install NGINX] ***********************************************************************************************************************************
ok: [node2.tp2.cloud]
ok: [node1.tp2.cloud]

TASK [nginx : Main NGINX config file] **************************************************************************************************************************
ok: [node1.tp2.cloud]
ok: [node2.tp2.cloud]

TASK [nginx : Create webroot] **********************************************************************************************************************************
ok: [node2.tp2.cloud]
ok: [node1.tp2.cloud]

TASK [nginx : Create index] ************************************************************************************************************************************
ok: [node2.tp2.cloud]
ok: [node1.tp2.cloud]

TASK [nginx : NGINX Virtual Host] ******************************************************************************************************************************
changed: [node1.tp2.cloud]
changed: [node2.tp2.cloud]

PLAY RECAP *****************************************************************************************************************************************************
node1.tp2.cloud            : ok=8    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2.tp2.cloud            : ok=8    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

```



## 5. GÃ©rer la suppression

DÃ©ployer et ajouter des trucs c'est bien beau, mais comment on fait pour gÃ©rer le changement ?

**Bah c'est la galÃ¨re.** Et il faut de la rigueur.

âžœ **CrÃ©ez un fichier qui permet de supprimer des Virtual Hosts NGINX**

```yaml
âžœ  tp2-ansible git:(master) âœ— cat Ansible/roles/nginx/tasks/add_vhosts.yml 
- name: Create webroot
  become: yes
  file:
    path: "{{ add_vhosts['nginx_webroot'] }}"
    state: directory

- name: Create index
  become: yes
  copy:
    dest: "{{ add_vhosts['nginx_webroot'] }}/index.html"
    content: "{{ add_vhosts['nginx_index_content'] }}"

- name: NGINX Virtual Host
  become: yes
  template:
    src: vhost.conf.j2
    dest: /etc/nginx/conf.d/{{ add_vhosts['nginx_servername'] }}.conf
```

```yaml
âžœ  tp2-ansible git:(master) âœ— cat Ansible/roles/nginx/tasks/remove_vhosts.yml 
- name: Remove index
  become: yes
  file:
    path: "{{ remove_vhosts['nginx_webroot'] }}/index.html"
    state: absent

- name: Remove NGINX Virtual Host
  become: yes
  file: 
    path: /etc/nginx/conf.d/{{ remove_vhosts['nginx_servername'] }}.conf
    state: absent
```



- testez que vous pouvez facilement ajouter ou supprimer des Virtual Hosts depuis le fichier `host_vars` d'une machine donnÃ©e

```yaml
âžœ  tp2-ansible git:(master) âœ— cat Ansible/inventories/vagrant_lab/host_vars/node1.tp2.cloud.yml 
common_packages:
  - vim
  - git
  - rsync

add_vhosts:
  nginx_servername: testnode3
  nginx_port: 8080
  nginx_webroot: /var/www/html/testnode3
  nginx_index_content: "<h1>teeeeeestnode3</h1>"

remove_vhosts:
  nginx_servername: testnode3
  nginx_port: 8080
  nginx_webroot: /var/www/html/testnode3
  nginx_index_content: "<h1>teeeeeestnode3</h1>"%
```

```yaml
âžœ  tp2-ansible git:(master) âœ— cat Ansible/inventories/vagrant_lab/host_vars/node2.tp2.cloud.yml
common_packages:
  - vim
  - git
  - rsync

add_vhosts:
  nginx_servername: testnode4
  nginx_port: 8080
  nginx_webroot: /var/www/html/testnode4
  nginx_index_content: "<h1>teeeeeestnode4</h1>"

remove_vhosts:
  nginx_servername:
  nginx_port:
  nginx_webroot: 
  nginx_index_content: %  
```

```bash
âžœ  Ansible git:(master) âœ— ansible-playbook -i inventories/vagrant_lab/hosts.ini playbooks/main.yml 

PLAY [ynov] ***************************************************************************************************************************************************************************

TASK [Gathering Facts] ****************************************************************************************************************************************************************
ok: [node1.tp2.cloud]
ok: [node2.tp2.cloud]

TASK [common : Install common packages] ***********************************************************************************************************************************************
ok: [node1.tp2.cloud] => (item=vim)
ok: [node2.tp2.cloud] => (item=vim)
ok: [node1.tp2.cloud] => (item=git)
ok: [node2.tp2.cloud] => (item=git)
ok: [node1.tp2.cloud] => (item=rsync)
ok: [node2.tp2.cloud] => (item=rsync)

TASK [common : Create a users attached to ynov group] *********************************************************************************************************************************
ok: [node1.tp2.cloud] => (item=le_nain)
ok: [node2.tp2.cloud] => (item=le_nain)
ok: [node1.tp2.cloud] => (item=l_elfe)
ok: [node2.tp2.cloud] => (item=l_elfe)
ok: [node2.tp2.cloud] => (item=le_ranger)
ok: [node1.tp2.cloud] => (item=le_ranger)

TASK [nginx : Install NGINX] **********************************************************************************************************************************************************
ok: [node1.tp2.cloud]
ok: [node2.tp2.cloud]

TASK [nginx : Main NGINX config file] *************************************************************************************************************************************************
ok: [node2.tp2.cloud]
ok: [node1.tp2.cloud]

TASK [nginx : Create webroot] *********************************************************************************************************************************************************
ok: [node2.tp2.cloud]
ok: [node1.tp2.cloud]

TASK [nginx : Create index] ***********************************************************************************************************************************************************
ok: [node2.tp2.cloud]
changed: [node1.tp2.cloud]

TASK [nginx : NGINX Virtual Host] *****************************************************************************************************************************************************
ok: [node2.tp2.cloud]
changed: [node1.tp2.cloud]

TASK [nginx : Remove index] ***********************************************************************************************************************************************************
changed: [node1.tp2.cloud]
ok: [node2.tp2.cloud]

TASK [nginx : Remove NGINX Virtual Host] **********************************************************************************************************************************************
changed: [node1.tp2.cloud]
ok: [node2.tp2.cloud]

PLAY RECAP ****************************************************************************************************************************************************************************
node1.tp2.cloud            : ok=10   changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2.tp2.cloud            : ok=10   changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
```



# III. Repeat

## 1. NGINX

âžœ **On reste dans le rÃ´le `nginx`**, faites en sorte que :

- on puisse dÃ©clarer la liste `vhosts` en *host_vars*
- si cette liste contient plusieurs `vhosts`, le rÃ´le les dÃ©ploie tous (exemple en dessous)
- le port prÃ©cisÃ© est automatiquement ouvert dans le firewall
- vous gÃ©rez explicitement les permissions de tous les fichiers

Exemple de fichier de variable avec plusieurs Virtual Hosts dans la liste `vhosts` :

```yaml
vhosts:
  - test2:
    nginx_servername: test2
    nginx_port: 8082
    nginx_webroot: /var/www/html/test2
    nginx_index_content: "<h1>teeeeeest 2</h1>"
  - test3:
    nginx_servername: test3
    nginx_port: 8083
    nginx_webroot: /var/www/html/test3
    nginx_index_content: "<h1>teeeeeest 3</h1>"
```



âžœ **Ajoutez une mÃ©canique de `handlers/`**

- c'est un nouveau dossier Ã  placer dans le rÃ´le
- je vous laisse dÃ©couvrir la mÃ©canique par vous-mÃªmes et la mettre en place
- vous devez trigger un *handler* Ã  chaque fois que la conf NGINX est modifiÃ©e
- vÃ©rifiez le bon fonctionnement
  - vous pouvez voir avec un `systemctl status` depuis quand une unitÃ© a Ã©tÃ© redÃ©marrÃ©e

## 2. Common

âžœ **On revient sur le rÃ´le `common`**, les utilisateurs dÃ©ployÃ©s doivent** :

- avoir un password
- avoir un homedir
- avoir accÃ¨s aux droits de `root` *via* `sudo`
- Ãªtre dans un groupe `admin`
- avoir une clÃ© SSH publique dÃ©posÃ© dans leur `authorized_keys`

> Toutes ces donnÃ©es doivent Ãªtre stockÃ©es dans les `group_vars`.

## 3. Dynamic loadbalancer

âžœ  **CrÃ©ez un nouveau rÃ´le : `webapp`**

- ce rÃ´le dÃ©ploie une application Web de votre choix, peu importe
- elle dÃ©ploie aussi le serveur web nÃ©cessaire pour que Ã§a tourne
  - vous pouvez clairement rÃ©utiliser le rÃ´le NGINX d'avant qui dÃ©ploie une bÃªte page HTML

> Vraiment, peu importe, une bÃªte page HTML, ou un truc open source comme un NextCloud. Ce qu'on veut, c'est simplement une interface visible.

âžœ  **CrÃ©ez un nouveau rÃ´le : `rproxy` (pour \*reverse proxy\*)**

- ce rÃ´le dÃ©ploie un NGINX
- NGINX est automatiquement configurÃ© pour agir comme un reverse proxy vers une liste d'IP qu'on lui fournit en variables
  - Ã  priori, vous allez gÃ©rer Ã§a avec des `host_vars` et `group_vars`

âžœ **Effectuez le dÃ©ploiement suivant :**

- deux machines portent le rÃ´le `webapp`
- une machine porte le rÃ´le `rproxy`
- faites en sorte que :
  - si on dÃ©ploie une nouvelle machine qui porte le rÃ´le `webapp`, la conf du reverse proxy se met Ã  jour en fonction
  - si on supprime une machine `webapp`, la conf du reverse proxy se met aussi Ã  jour en fonction

> La configuration de votre loadbalancer devient dynamique, et plus aucune connexion manuelle n'est nÃ©cessaire pour ajuster la taille du parc en fonction de la charge.

# IV. Bonus : Aller plus loin

## 1. Vault Ansible

Afin de ne pas stocker de donnÃ©es sensibles en clair dans les fichiers Ansible, comme des mots de passe, on peut utiliser les [vault Ansible](https://docs.ansible.com/ansible/latest/user_guide/vault.html).

Cela permet de stocker ces donnÃ©es, mais dans des fichiers chiffrÃ©s, Ã  l'intÃ©rieur du dÃ©pÃ´t Ansible.

âžœ **Utilisez les Vaults pour stocker les clÃ©s publiques des utilisateurs**

## 2. Support de plusieurs OS

**Il est possible qu'un rÃ´le donnÃ© fonctionne pour plusieurs OS.** Pour Ã§a, on va utiliser des conditions en fonction de l'OS de la machine de destination.

A chaque fois qu'on dÃ©ploie de la conf sur une machine, cette derniÃ¨re nous donne beaucoup d'informations Ã  son sujet : ses ***facts***. Par exemple, on rÃ©cupÃ¨re la liste des cartes rÃ©seau de la machine, la liste des utilisateurs, l'OS utilisÃ©, etc.

On peut alors rÃ©cupÃ©rer ces variables dans nos tasks, pour les insÃ©rer dans des templates par exemple, ou encore effectuer du travail conditionnel :

```yaml
  - name: Install apache
    apt: 
      name: apache
      state: latest
    when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'

  - name: Install apache
    yum: 
      name: httpd # le nom du paquet est diffÃ©rent sous CentOS
      state: latest
    when: ansible_distribution == 'CentOS'
```



âžœ **Ajoutez une machine d'un OS diffÃ©rent Ã  votre `Vagrantfile` et adaptez vos playbooks**

- passez sur une CentOS si vous Ã©tiez sur une base Debian jusqu'alors
- ou vice-versa